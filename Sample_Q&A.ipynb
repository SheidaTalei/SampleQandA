{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sample_Q&A",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsj6u4mU5Nt/W1l6lpILih",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheidaTalei/SampleQandA/blob/main/Sample_Q%26A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7QON48OTyYX",
        "outputId": "fbb18c64-f75b-4ec8-b5a2-d6d75ec9361e"
      },
      "source": [
        "import os, sys\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "nb_path = '/content/notebooks'\n",
        "os.symlink('/content/drive/My Drive/Colab Notebooks', nb_path)\n",
        "sys.path.insert(0,nb_path)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f6OcW99a6nd"
      },
      "source": [
        "#!pip install --target=$nb_path hazm\n",
        "# !pip install --target=$nb_path persianutils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7KSrQjZaw9U"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import regex\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from hazm import *\n",
        "import re\n",
        "import string\n",
        "import persianutils as pu\n",
        "import persianutils.PersianAlphabet as pALpha"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCtK2GHWUALW"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import Conv1D\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from sklearn.metrics import  auc"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACBTMFnHfEuq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "#Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import regex\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import re\n",
        "import string"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSqy1EggaqJ2"
      },
      "source": [
        "#---------------------------------------------------- Latin and Punctuation Removal-------------------------------------------\n",
        "# This function will: 1- Remove latin words 2- Remove punctuations\n",
        "# Source: https://stackoverflow.com/questions/18429143/strip-punctuation-with-regex-python/50985687\n",
        "def latinRemoval(content):\n",
        "    only_persian_list =[]\n",
        "    temp_set = []\n",
        "\n",
        "    latin = re.compile(r'[a-zA-Z0-9@$!%*?&-_`.+#(»)«,:;،؛…..؟]+')  \n",
        "\n",
        "    for item in range(len(content)):\n",
        "        for word in content[item]:\n",
        "            temp_word = word.replace('\\u200c', ' ', 1000000000)\n",
        "               \n",
        "            if not(latin.match(temp_word)) :\n",
        "                if not temp_word.isdigit():\n",
        "                        #source: https://pypi.org/project/persianutils/\n",
        "                    word = re.findall(r'[^۰-۹]+', temp_word.strip(string.punctuation))\n",
        "                    \n",
        "                    if len(word) >0:    \n",
        "                        processed_text = pu.standardize4Word2vec(word[0].strip())\n",
        "\n",
        "                        if (processed_text != ' '):\n",
        "#                             print(processed_text)\n",
        "                            word = re.findall('\\d+', processed_text.strip())\n",
        "                            if len(word)==0 : \n",
        "                                temp_set.append(processed_text.strip())\n",
        "                                # print (processed_text.strip())\n",
        "           \n",
        "        only_persian_list.append(set(temp_set))\n",
        "        temp_set = []\n",
        "        \n",
        "    return only_persian_list"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7F-u17HbxKU"
      },
      "source": [
        "#--------------------------------------------------Loading StopWords ------------------------------------------\n",
        "#Source of file: https://sites.google.com/site/kevinbouge/stopwords-lists\n",
        "def getStopWord ():\n",
        "    try:\n",
        "        file = open('/content/drive/MyDrive/Final/stopwords_fa.txt', 'r', encoding='utf-8-sig')\n",
        "        file_readed = file.read()\n",
        "    \n",
        "    finally:\n",
        "        file.close()\n",
        "        \n",
        "    stopWord_Set = set(file_readed.split())\n",
        "    return stopWord_Set\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZByXQlAab0iY"
      },
      "source": [
        "#------------------------------------Tokenization and Stop Word Removal------------------------------------------------------\n",
        "# This function will: 1- Tokenize words  2- Remove stop words and return a list of sets\n",
        "def getNoStopWord(content):\n",
        "    no_StopWord_list =[]\n",
        "    temp_set = []\n",
        "    stop_word_list = getStopWord()\n",
        "    for item in range(len(content)):\n",
        "        for word in content[item]:\n",
        "            if not word in stop_word_list:\n",
        "                temp_set.append(word.replace('\\u200c', ' ', 1000000000))\n",
        "                \n",
        "        no_StopWord_list.append(set (temp_set))\n",
        "        temp_set = []\n",
        "        \n",
        "    return no_StopWord_list"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9VEoS9tb1zV"
      },
      "source": [
        "#----------------------------------------------Lemmatization ------------------------------------------------------------------\n",
        "def Lemmatization (content):\n",
        "    lemmatizer = Lemmatizer()\n",
        "    lemmatize_list = []\n",
        "    temp_set = []\n",
        "    for item in range(len(content)):\n",
        "        for word in (content[item]):\n",
        "            temp_set.append(lemmatizer.lemmatize(word.replace('\\u200c', ' ', 1000000000)))\n",
        "        lemmatize_list.append(set(temp_set))\n",
        "        temp_set = []\n",
        "    \n",
        "    return lemmatize_list"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFK5iG7b78q"
      },
      "source": [
        "#---------------------------------------------------Tokenization-------------------------------------------------------------\n",
        "# Tokenizing X and at the end append Y as a label\n",
        "def tokenization (X):\n",
        "    tokenized_train_data = []\n",
        "    for item in range(len(X)):\n",
        "        var = word_tokenize(X[item])\n",
        "#         var.append(Y[item])\n",
        "        tokenized_train_data.append(var)\n",
        "        \n",
        "    return tokenized_train_data   "
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO-O8bgocDch"
      },
      "source": [
        "#Loading sample Dataset\n",
        "#Here is a training dataset\n",
        "dataFarme_train = pd.read_csv ('/content/drive/MyDrive/temp_QandA/SampleDataSet_Train_V0.csv', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "Y= dataFarme_train.label\n",
        "X = dataFarme_train.question  + \" \"+  dataFarme_train.answer"
      ],
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8HfCuSwpJjL",
        "outputId": "d320ed2d-8212-48e0-eb75-3edeb5159de6"
      },
      "source": [
        "print(type(X))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012Afw8EfK-R"
      },
      "source": [
        "#Tokenization\n",
        "tokenize_list = tokenization (X)"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtyq9IklfK1p"
      },
      "source": [
        "only_persian_list = latinRemoval(tokenize_list)"
      ],
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1hm6V1tfvDZ"
      },
      "source": [
        "noStopWord_List = getNoStopWord(only_persian_list)"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz2pqZ3_f1ws",
        "outputId": "df41d55c-f459-4f58-f75f-d4ae676a458e"
      },
      "source": [
        "lemmatize_list = Lemmatization (noStopWord_List)\n",
        "print (type(lemmatize_list))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJO4vbsKk5K6"
      },
      "source": [
        "def convertSequenceWordsToText(input_list,  fileName, lastFileName):\n",
        "    X_and_Y = pd.read_csv(fileName , encoding=\"utf-8\")\n",
        "    text = ''\n",
        "    counter = 0\n",
        "    updator = 0\n",
        "    for item in range(len(input_list)):\n",
        "      for i in range(len(input_list [item + updator])):\n",
        "        text += list(input_list[item + updator])[i] + ' '\n",
        "      \n",
        "      \n",
        "      print(counter)    \n",
        "      X_and_Y.loc [counter, 'text'] = text.strip()\n",
        "      X_and_Y.to_csv(lastFileName ,index=False, encoding='utf-8-sig')\n",
        "      counter = counter + 1\n",
        "      text =''\n",
        "    print(\"This is counter: \" ,counter-1)"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_go0UXflFVC"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# define a list of places\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/temp_QandA/lemmatize_list.pkl\"\n",
        "open_file = open(file_name, \"wb\")\n",
        "pickle.dump(lemmatize_list, open_file)\n",
        "open_file.close()\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/temp_QandA/lemmatize_list.pkl\"\n",
        "open_file = open(file_name, \"rb\")\n",
        "lemmatize_list = pickle.load(open_file)\n",
        "open_file.close()\n"
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MPPFl_clT6I",
        "outputId": "487d2f75-7f35-40ce-904c-b9b2d0e5b20f"
      },
      "source": [
        "convertSequenceWordsToText(lemmatize_list,'/content/drive/MyDrive/temp_QandA/SampleDataSet_Train_V0.csv', '/content/drive/MyDrive/temp_QandA/Prepared_Data.csv')"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "This is counter:  11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcrMcSjvllW4"
      },
      "source": [
        "dataFarme_train = pd.read_csv ('/content/drive/MyDrive/temp_QandA/Prepared_Data.csv', encoding='utf-8-sig')"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PFVb1RFlwZS"
      },
      "source": [
        "Y= dataFarme_train.label\n",
        "X = dataFarme_train.text"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdfllSgdgCJx"
      },
      "source": [
        "#converting words to vectors\n",
        "max_len = 10 \n",
        "tokenizer = Tokenizer(num_words = max_len, lower = True, split = ' ')\n",
        "tokenizer.fit_on_texts(X) \n",
        "\n",
        "\n",
        "X_tokenize = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "#pad sequence is used to ensure that all sequences in a list have the same length.\n",
        "X_ready = pad_sequences(sequences = X_tokenize, maxlen = max_len, padding = 'pre')"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKgaf19BgF2u"
      },
      "source": [
        "def createLstmModel(units = 100, dropout = 0.5  ):\n",
        "    \n",
        "    \n",
        "   #https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
        "        \n",
        "    #Adding Layers\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(100 , 128 ))#max_feature = 5000 , embedding_dimention = 128\n",
        "\n",
        "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))  \n",
        "   \n",
        "    # model.add(LSTM(units = units , dropout = 0.2, return_sequences=True ))\n",
        "    # model.add(Dense(units = units, activation=\"relu\"))\n",
        "    # model.add(Dropout(rate = dropout))\n",
        "\n",
        "\n",
        "  \n",
        "    model.add(LSTM(units = units , dropout = 0.2))\n",
        "    model.add(Dense(units = units, activation=\"relu\"))\n",
        "    model.add(Dropout(rate = dropout))\n",
        "\n",
        "    model.add(Dense(units = 1 ,activation=\"sigmoid\"))\n",
        "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=[ 'accuracy'])\n",
        "    \n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywpsxgkegFv5",
        "outputId": "eeedb9ce-de82-429e-fd4c-1cff63c1b4cc"
      },
      "source": [
        "model = createLstmModel()\n",
        "\n",
        "model.fit(X_ready, Y,  epochs=20,batch_size = 4)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6948 - accuracy: 0.4167\n",
            "Epoch 2/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6988 - accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6946 - accuracy: 0.5833\n",
            "Epoch 4/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6854 - accuracy: 0.5833\n",
            "Epoch 5/20\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6778 - accuracy: 0.5833\n",
            "Epoch 6/20\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6681 - accuracy: 0.5833\n",
            "Epoch 7/20\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6533 - accuracy: 0.5833\n",
            "Epoch 8/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6377 - accuracy: 0.5833\n",
            "Epoch 9/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5991 - accuracy: 0.5833\n",
            "Epoch 10/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5820 - accuracy: 0.7500\n",
            "Epoch 11/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5527 - accuracy: 0.6667\n",
            "Epoch 12/20\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4882 - accuracy: 0.6667\n",
            "Epoch 13/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4492 - accuracy: 0.8333\n",
            "Epoch 14/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4378 - accuracy: 0.8333\n",
            "Epoch 15/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3987 - accuracy: 0.9167\n",
            "Epoch 16/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3463 - accuracy: 0.8333\n",
            "Epoch 17/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2823 - accuracy: 0.9167\n",
            "Epoch 18/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3803 - accuracy: 0.7500\n",
            "Epoch 19/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2258 - accuracy: 0.9167\n",
            "Epoch 20/20\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2831 - accuracy: 0.8333\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8b5b7d4780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKGUjnNnhWOt"
      },
      "source": [
        ""
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIcIOCMfhWL2"
      },
      "source": [
        "#Here is Testing"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znDw7o3nhWJO"
      },
      "source": [
        "#Loading sample Dataset\n",
        "#Here is a training dataset\n",
        "dataFarme_test = pd.read_csv ('/content/drive/MyDrive/temp_QandA/SampleDataSet_Test.csv', encoding='utf-8-sig')\n",
        "\n",
        "\n",
        "Y_test= dataFarme_test.label\n",
        "X_test = dataFarme_test.question  + \" \"+  dataFarme_test.answet"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y7hdxVWhWGs"
      },
      "source": [
        "tokenize_list_test = tokenization (X_test)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb5_6xLnhWEo"
      },
      "source": [
        "only_persian_list_test = latinRemoval(tokenize_list_test)"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT6f6Thlixbq"
      },
      "source": [
        "noStopWord_List_test = getNoStopWord(only_persian_list_test)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCZg5QJxixWK"
      },
      "source": [
        "lemmatize_list_test = Lemmatization (noStopWord_List_test)"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZB8VrYzixN2"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# define a list of places\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/temp_QandA/lemmatize_list_test.pkl\"\n",
        "open_file = open(file_name, \"wb\")\n",
        "pickle.dump(lemmatize_list_test, open_file)\n",
        "open_file.close()\n",
        "\n",
        "file_name = \"/content/drive/MyDrive/temp_QandA/lemmatize_list_test.pkl\"\n",
        "open_file = open(file_name, \"rb\")\n",
        "lemmatize_list_test = pickle.load(open_file)\n",
        "open_file.close()"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLdCMYn0mrxn",
        "outputId": "1b5d5f5f-38e9-4dc5-a0c5-d23612b5819b"
      },
      "source": [
        "convertSequenceWordsToText(lemmatize_list_test,'/content/drive/MyDrive/temp_QandA/SampleDataSet_Test.csv', '/content/drive/MyDrive/temp_QandA/Prepared_Data_test.csv')"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "This is counter:  3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sS80UgYcmrv0"
      },
      "source": [
        "dataFarme_test = pd.read_csv ('/content/drive/MyDrive/temp_QandA/Prepared_Data.csv', encoding='utf-8-sig')\n",
        "Y_test= dataFarme_test.label\n",
        "X_test = dataFarme_test.text"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XceGOpTrmrs_"
      },
      "source": [
        ""
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4SKmeEjmriw"
      },
      "source": [
        ""
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ymmq70hUgFnx"
      },
      "source": [
        "#converting words to vectors\n",
        "tokenizer.fit_on_texts(X_test )\n",
        "\n",
        "X_temp = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "#pad sequence is used to ensure that all sequences in a list have the same length.\n",
        "X_ready_test = pad_sequences(sequences = X_temp, maxlen = max_len, padding = 'pre')"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2cGmBg6jAtw",
        "outputId": "d811f013-1ab5-461c-83f5-47fdecf88722"
      },
      "source": [
        "loss, accuracy = model.evaluate(X_ready_test, Y_test, verbose=0)\n",
        "print('Accuracy:  %.3f' , accuracy)\n",
        "\n"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy:  %.3f 0.9166666865348816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0pLUFXQjbHZ"
      },
      "source": [
        ""
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1NSMOSvjcMK"
      },
      "source": [
        "#THRILLING part"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJRb6ZOWjfp7"
      },
      "source": [
        "#Write your Question here ? greeting or music\n",
        "Question = \"امروز، حالت خوبه؟\"\n",
        "# Question= \"موسیقی چی گوش میدی؟\"\n",
        "tokenize_list_test1 =tokenization (pd.Series(Question))\n",
        "only_persian_list_test1 = latinRemoval(tokenize_list_test1)\n",
        "noStopWord_List_test1 = getNoStopWord(only_persian_list_test1)\n",
        "lemmatize_list_test1 = Lemmatization (noStopWord_List_test1)"
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhumvduonuSI",
        "outputId": "6d30b9cd-5a42-4cd4-e537-98697be28d73"
      },
      "source": [
        "print(lemmatize_list_test1)"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'حالت', 'خوبه'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKJbyBhSnMOJ"
      },
      "source": [
        "\n",
        "temp_var = \"\"\n",
        "for item in range(len(lemmatize_list_test1)):\n",
        "  for i in range(len(lemmatize_list_test1[item])):\n",
        "    temp_var += list (lemmatize_list_test1[item])[i] + \" \""
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BisVQ2qHnnek",
        "outputId": "f54bbabe-0897-42e9-ec8b-60b922a626d3"
      },
      "source": [
        "print(temp_var)"
      ],
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "حالت خوبه \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LhEooeRrFXx",
        "outputId": "10780963-7f83-4629-efdb-8d9be3b0f644"
      },
      "source": [
        "#converting words to vectors\n",
        "\n",
        "tokenizer.fit_on_texts(temp_var )\n",
        "\n",
        "X_temp_test1 = tokenizer.texts_to_sequences(temp_var)\n",
        "print(X_temp_test1)\n",
        "#pad sequence is used to ensure that all sequences in a list have the same length.\n",
        "X_ready_test1 = pad_sequences(sequences = X_temp_test1, maxlen = max_len, padding = 'pre')"
      ],
      "execution_count": 255,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3], [4], [5], [1], [], [6], [2], [7], [8], []]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx3ROH3Hrme5"
      },
      "source": [
        ""
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmdZdcScqO10"
      },
      "source": [
        "y_predict =model.predict_classes(X_ready_test1)"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kx9CNriq9GZ",
        "outputId": "4b183238-8907-4c75-ac6f-94adf2a1b0a1"
      },
      "source": [
        "print( y_predict)"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wH_mgt9MrVwj",
        "outputId": "ec311d05-c560-45ee-adbd-87ee431d52b4"
      },
      "source": [
        "count_0 = 0\n",
        "count_1= 0\n",
        "for i in y_predict:\n",
        "  if i == 0:\n",
        "    count_0 +=1\n",
        "  else: \n",
        "    count_1 +=1\n",
        "\n",
        "\n",
        "if count_1 > count_0:\n",
        "  print(1)\n",
        "else:\n",
        "    print(0)\n",
        "\n",
        "\n"
      ],
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWwgvIultrcd"
      },
      "source": [
        ""
      ],
      "execution_count": 258,
      "outputs": []
    }
  ]
}